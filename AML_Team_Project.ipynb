{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qo5tKm3Zyv2T"
   },
   "source": [
    "EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GJ3Pwkw1yv2V"
   },
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_auc_score, roc_curve, auc, precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score, f1_score, precision_score, recall_score, ConfusionMatrixDisplay\n",
    "from statsmodels.graphics.mosaicplot import mosaic\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.pipeline import make_pipeline as imb_make_pipeline\n",
    "from collections import Counter\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_Du6RYoyv2V"
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "df_customer_churn = pd.read_excel('C:/Users/xgygr/Desktop/CustomerChurn.xlsx')\n",
    "df_telco_customer_churn = pd.read_excel('C:/Users/xgygr/Desktop/Telco_customer_churn.xlsx')\n",
    "df_telco_customer_churn_demographics = pd.read_excel('C:/Users/xgygr/Desktop/Telco_customer_churn_demographics.xlsx')\n",
    "df_telco_customer_churn_location = pd.read_excel('C:/Users/xgygr/Desktop/Telco_customer_churn_location.xlsx')\n",
    "df_telco_customer_churn_population = pd.read_excel('C:/Users/xgygr/Desktop/Telco_customer_churn_population.xlsx')\n",
    "df_telco_customer_churn_services = pd.read_excel('C:/Users/xgygr/Desktop/Telco_customer_churn_services.xlsx')\n",
    "df_telco_customer_churn_status = pd.read_excel('C:/Users/xgygr/Desktop/Telco_customer_churn_status.xlsx')\n",
    "\n",
    "# Rename the columns so that the primary key is the same\n",
    "df_telco_customer_churn.rename(columns = {'CustomerID':'Customer ID'}, inplace = True)\n",
    "\n",
    "# drop the count column from all datasets\n",
    "df_telco_customer_churn.drop(['Count'], axis=1, inplace=True)\n",
    "df_telco_customer_churn_demographics.drop(['Count'], axis=1, inplace=True)\n",
    "df_telco_customer_churn_location.drop(['Count'], axis=1, inplace=True)\n",
    "df_telco_customer_churn_services.drop(['Count'], axis=1, inplace=True)\n",
    "df_telco_customer_churn_status.drop(['Count'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GAik5sZAyv2W",
    "outputId": "7fbdd7a9-2baa-4c3c-a9ad-ba31d24ab60c"
   },
   "outputs": [],
   "source": [
    "# Print the columns of each dataset\n",
    "print(df_customer_churn.columns)\n",
    "print(df_telco_customer_churn.columns)\n",
    "print(df_telco_customer_churn_demographics.columns)\n",
    "print(df_telco_customer_churn_location.columns)\n",
    "print(df_telco_customer_churn_population.columns)\n",
    "print(df_telco_customer_churn_services.columns)\n",
    "print(df_telco_customer_churn_status.columns)\n",
    "\n",
    "# count rows for each dataset\n",
    "print(df_customer_churn.shape)\n",
    "print(df_telco_customer_churn.shape)\n",
    "print(df_telco_customer_churn_demographics.shape)\n",
    "print(df_telco_customer_churn_location.shape)\n",
    "print(df_telco_customer_churn_population.shape)\n",
    "print(df_telco_customer_churn_services.shape)\n",
    "print(df_telco_customer_churn_status.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vdI6CBdnyv2W"
   },
   "outputs": [],
   "source": [
    "#Merge datasets except for df_telco_customer_churn_population into one big dataset using CustomerID as the key, and remove duplicate columns\n",
    "# Merge datasets into one big dataset using CustomerID as the key\n",
    "df = pd.merge(df_telco_customer_churn, df_telco_customer_churn_demographics, on='Customer ID', how='left')\n",
    "df = pd.merge(df, df_telco_customer_churn_location, on='Customer ID', how='left')\n",
    "df = pd.merge(df, df_telco_customer_churn_services, on='Customer ID', how='left')\n",
    "df = pd.merge(df, df_telco_customer_churn_status, on='Customer ID', how='left')\n",
    "\n",
    "\n",
    "# Remove _x and _y suffix, and drop duplicate columns\n",
    "df.columns = df.columns.str.replace('_x', '')\n",
    "df.columns = df.columns.str.replace('_y', '')\n",
    "\n",
    "# Identify and drop duplicate columns\n",
    "df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "# count distinct column values for each column\n",
    "df.nunique()\n",
    "\n",
    "# Drop unnecessary columns: (too granular, don't provide additional information, or similar to other category)\n",
    "df.drop(['Customer ID','Country','Lat Long', 'Latitude','City','State','Zip Code','Location ID','Service ID','Status ID', 'Longitude', 'Churn Label','Quarter','Internet Type', 'Churn Reason'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CneykRpGyv2X",
    "outputId": "839f8ef1-db9e-41e3-b117-d91de0d78b1d"
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "# Check for missing values in df\n",
    "missing_values = df.isnull().sum()\n",
    "rows_with_missing_values = df.isnull().any(axis=1).sum()\n",
    "\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    # Check for missing values in df\n",
    "    missing_values = df.isnull().sum()\n",
    "    rows_with_missing_values = df.isnull().any(axis=1).sum()\n",
    "\n",
    "    # Display the results\n",
    "    print(\"Categories with Missing Values:\")\n",
    "    print(missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IWhgUf73yv2X"
   },
   "outputs": [],
   "source": [
    "# Rows that have missing values: Total Charges, Offer, Churn Reason, Churn Category\n",
    "# Drop rows with missing total charges since there are only 11 of them\n",
    "df = df.dropna(subset=['Total Charges'])\n",
    "\n",
    "# fill null values with 'None' when a customer didn't receive any offers\n",
    "df['Offer'] = df['Offer'].fillna('None')\n",
    "\n",
    "# fill null values with 'did not churn' for Churn Category\n",
    "df['Churn Category'] = df['Churn Category'].fillna('Did Not Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 686
    },
    "id": "nE4mcrDQyv2X",
    "outputId": "8654fa26-c385-40a0-b1a8-966baa49f89a"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mI4cpSzoyv2X"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 'Total Refunds', 'Total Extra Data Charges', 'Tenure in Months'\n",
    "numerical_cols = ['Tenure Months', 'Monthly Charges', 'Total Charges', 'CLTV', 'Age', 'Avg Monthly Long Distance Charges',\n",
    " 'Avg Monthly GB Download', 'Monthly Charge', 'Total Long Distance Charges', 'Total Revenue']\n",
    "\n",
    "categorical_cols = ['Gender', 'Senior Citizen', 'Partner', 'Dependents', 'Phone Service', 'Multiple Lines', 'Internet Service',\n",
    " 'Online Security', 'Online Backup', 'Device Protection', 'Tech Support', 'Streaming TV', 'Streaming Movies', 'Contract', 'Paperless Billing', 'Payment Method',\n",
    " 'Under 30', 'Married', 'Referred a Friend', 'Offer', 'Device Protection Plan', 'Premium Tech Support', 'Streaming Music', 'Unlimited Data','Satisfaction Score',\n",
    " 'Number of Dependents', 'Number of Referrals']\n",
    "\n",
    "Other_Churn_Flags =['Churn Score', 'Churn Value', 'Customer Status', 'Churn Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "uxPt63ubyv2Y",
    "outputId": "679d9470-ee5d-4f0c-e5fb-6698c91cf12d"
   },
   "outputs": [],
   "source": [
    "df['Churn Value'] = df['Churn Value'].apply(lambda x: 'Churned' if x == 1 else 'Not Churned')\n",
    "df['Churn Value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 885
    },
    "id": "k9sJcLAoyv2Y",
    "outputId": "cfbeaa4f-1930-4636-f56b-e1739ac94a4e"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 10))\n",
    "churn_counts = df['Churn Value'].value_counts()\n",
    "\n",
    "axes[0].bar(churn_counts.index, churn_counts.values, color = [sns.color_palette('Set1')[1], sns.color_palette('Set1')[0]])\n",
    "axes[0].set_title(\"Distribution of Churn Value\", fontsize=16)\n",
    "axes[0].set_xlabel(\"Churn Value\", fontsize=14)\n",
    "axes[0].set_ylabel(\"Count\", fontsize=14)\n",
    "axes[0].set_xticklabels(['Did Not Churned (0)', 'Churned (1)'])\n",
    "\n",
    "axes[1].pie(churn_counts.tolist(), labels = churn_counts.index.tolist(), colors = [sns.color_palette('Set1')[1],\n",
    "                                                                                   sns.color_palette('Set1')[0]],\n",
    "         autopct='%.1f%%', startangle = 90)\n",
    "axes[1].set_title(f\"Pie Chart of Churn Value Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLWDDt7Gyv2Y"
   },
   "source": [
    "# Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-OnaIflNyv2Y",
    "outputId": "fce01651-82d7-43f5-d3c9-cba20fea6f9e"
   },
   "outputs": [],
   "source": [
    "num_categorical_cols = len(categorical_cols)\n",
    "num_rows_cat = math.ceil(num_categorical_cols / 2)\n",
    "custom_palette = {'Not Churned': sns.color_palette('Set1')[1], 'Churned': sns.color_palette('Set1')[0]}\n",
    "\n",
    "plt.figure(figsize=(14, num_rows_cat * 4))\n",
    "for idx, col in enumerate(categorical_cols):\n",
    "    plt.subplot(num_rows_cat, 2, idx + 1)\n",
    "    sns.countplot(data=df, x=col, hue='Churn Value', palette=custom_palette)\n",
    "    plt.title(f'Distribution of {col} by Churn')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "uoS52nevyv2Y",
    "outputId": "1c1e1a9d-9de8-410b-a485-d4ca482db063"
   },
   "outputs": [],
   "source": [
    "# https://www.statsmodels.org/stable/generated/statsmodels.graphics.mosaicplot.mosaic.html\n",
    "\n",
    "fig, axes = plt.subplots(9, 3, figsize=(30, num_rows_cat * 4))\n",
    "axes_idx = 0\n",
    "axes_graph_idx = 0\n",
    "\n",
    "for idx, col in enumerate(categorical_cols):\n",
    "    col_churn = df.groupby([col, 'Churn Value']).size().to_dict()\n",
    "    mosaic(col_churn, ax=axes[axes_idx][axes_graph_idx], title = f'Mosaic Plot of {col} by Churn', axes_label = True,\n",
    "           labelizer=lambda k: \"\")\n",
    "\n",
    "    axes_graph_idx += 1\n",
    "\n",
    "    if (axes_graph_idx % 3 == 0):\n",
    "        axes_graph_idx = 0\n",
    "        axes_idx += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "id": "lO_s_Ckcyv2Z",
    "outputId": "65fee2f7-c8a2-41b4-93e6-b9aed9ecf433"
   },
   "outputs": [],
   "source": [
    "pie_category = ['Churn Category', 'Satisfaction Score', 'Customer Status']\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 20))\n",
    "idx = 0\n",
    "\n",
    "for category in pie_category:\n",
    "    pie_category_freq = df[category].value_counts()\n",
    "\n",
    "    pie_category_key = pie_category_freq.index.tolist()\n",
    "    pie_category_data = pie_category_freq.tolist()\n",
    "\n",
    "    axes[idx].pie(pie_category_data, labels = pie_category_key, colors = sns.color_palette('bright'),\n",
    "         autopct='%.0f%%', startangle = 90)\n",
    "    axes[idx].set_title(f\"Pie Chart of {category} Distribution\")\n",
    "\n",
    "\n",
    "    idx += 1\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZdZE8JEyv2Z"
   },
   "source": [
    "# Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vr0tVK9pyv2Z"
   },
   "outputs": [],
   "source": [
    "# Function to clean numeric columns\n",
    "def clean_numeric_columns(df, columns):\n",
    "    for col in columns:\n",
    "        # Replace any spaces or empty strings with NaN\n",
    "        df[col] = df[col].replace(\" \", None)  # Replacing spaces with None\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')  # Convert to numeric, forcing invalid values to NaN\n",
    "    return df\n",
    "\n",
    "# Replace inf values with NaN across the entire dataframe\n",
    "df.replace([float('inf'), float('-inf')], float('nan'), inplace=True)\n",
    "\n",
    "# Clean numeric columns\n",
    "df = clean_numeric_columns(df, numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "IYO_gdD8yv2Z",
    "outputId": "db4d1827-7f1c-4517-836f-212cb1d7dfec"
   },
   "outputs": [],
   "source": [
    "df[numerical_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "OY13zPqvyv2Z",
    "outputId": "5d414c41-b830-4e0f-b53f-22b9e48c9e18"
   },
   "outputs": [],
   "source": [
    "# Plot distribution for numerical columns\n",
    "num_numerical_cols = len(numerical_cols)\n",
    "num_rows_num = math.ceil(num_numerical_cols / 2)\n",
    "\n",
    "plt.figure(figsize=(14, num_rows_num * 4))\n",
    "for idx, col in enumerate(numerical_cols):\n",
    "    plt.subplot(num_rows_num, 2, idx + 1)\n",
    "    sns.histplot(data=df, x=col, hue='Churn Value', element=\"step\", stat=\"density\", common_norm=False, palette=custom_palette)\n",
    "    plt.title(f'Distribution of {col} by Churn')\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3P3MVJZUyv2Z",
    "outputId": "6cfe332f-62ef-4044-e230-ef7766de17cb"
   },
   "outputs": [],
   "source": [
    "num_numerical_cols = len(numerical_cols)\n",
    "num_rows_num = math.ceil(num_numerical_cols / 2)\n",
    "\n",
    "plt.figure(figsize=(14, num_rows_num * 4))\n",
    "for idx, col in enumerate(numerical_cols):\n",
    "    plt.subplot(num_rows_num, 2, idx + 1)\n",
    "    sns.boxplot(data = df, x = \"Churn Value\", y= col, hue = \"Churn Value\", palette=custom_palette)\n",
    "    plt.title(f'Box Plot Distribution of {col} by Churn')\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "YlRBRFuVyv2a",
    "outputId": "66a9fac7-3e08-4b9f-8048-222ae04cb53f"
   },
   "outputs": [],
   "source": [
    "df[numerical_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numerical_cols:\n",
    "    print(f\"Before clipping, {col}: {df[col].describe()}\")\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    df[col] = df[col].apply(lambda x: min(max(x, lower_bound), upper_bound))\n",
    "    \n",
    "    print(f\"After clipping, {col}: {df[col].describe()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "def object_to_int(dataframe_series):\n",
    "    if dataframe_series.dtype=='object':\n",
    "        dataframe_series = LabelEncoder().fit_transform(dataframe_series)\n",
    "    return dataframe_series\n",
    "\n",
    "df = df.apply(lambda x: object_to_int(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1riMVEQIyv2a"
   },
   "source": [
    "# Data Splitting and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "id": "w4rpfuZYyv2a",
    "outputId": "539c1bb1-239f-4bb6-d399-01dc19615f0f"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,7))\n",
    "df.corr()['Churn Value'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TcKjkxpKyv2a"
   },
   "outputs": [],
   "source": [
    "df_x = df.drop(columns=['Churn Value'])\n",
    "df_y = df['Churn Value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QWUvnrDUyv2a"
   },
   "outputs": [],
   "source": [
    "# split to 80/20\n",
    "x_dev, x_test, y_dev, y_test = train_test_split(df_x, df_y, stratify = df_y, test_size = 0.2, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IPbmqprpyv2a",
    "outputId": "7d5d79f0-fa7c-4fff-d7ac-ddbee06c9dc4"
   },
   "outputs": [],
   "source": [
    "print(\"x_dev Shape: \", x_dev.shape)\n",
    "print(\"x_test Shape: \", x_test.shape, \"\\n\")\n",
    "\n",
    "print(\"y_dev Shape: \", y_dev.shape)\n",
    "print(\"y_test Shape: \", y_test.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fSoLW3xoyv2a"
   },
   "outputs": [],
   "source": [
    "# This was used to help decide what feature goes to what encoder\n",
    "# for categorical in categorical_cols:\n",
    "#     print(\"Category: \", categorical)\n",
    "#     print(len(sorted(df[categorical].unique())))\n",
    "#     print(sorted(df[categorical].unique()), \"\\n\")\n",
    "\n",
    "# for categorical in Other_Churn_Flags:\n",
    "#     print(\"Category: \", categorical)\n",
    "#     print(len(sorted(df[categorical].unique())))\n",
    "#     print(sorted(df[categorical].unique()), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j3yAPXgQyv2a"
   },
   "outputs": [],
   "source": [
    "ordinal_categorical = ['Contract', 'Offer', 'Satisfaction Score', 'Number of Dependents', 'Number of Referrals', 'Customer Status']\n",
    "one_hot_categorical = []\n",
    "\n",
    "for categorical in categorical_cols:\n",
    "    if categorical not in ordinal_categorical:\n",
    "        one_hot_categorical.append(categorical)\n",
    "\n",
    "one_hot_categorical.append('Churn Category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yugit17Iyv2a"
   },
   "outputs": [],
   "source": [
    "for ordinal_category in ordinal_categorical:\n",
    "    sorted_ordinal = [sorted(x_dev[ordinal_category].unique())]\n",
    "    ordinal_encoder = OrdinalEncoder(categories = sorted_ordinal)\n",
    "\n",
    "    x_dev[f\"{ordinal_category}_ord\"] = ordinal_encoder.fit_transform(x_dev[[ordinal_category]])\n",
    "    x_test[f\"{ordinal_category}_ord\"] = ordinal_encoder.transform(x_test[[ordinal_category]])\n",
    "\n",
    "x_dev = x_dev.drop(columns = ordinal_categorical)\n",
    "x_test = x_test.drop(columns = ordinal_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lzdsH0tyyv2b",
    "outputId": "5513f580-e3f5-4235-ea05-2e4bf178344c"
   },
   "outputs": [],
   "source": [
    "print(\"x_dev Shape: \", x_dev.shape)\n",
    "print(\"x_test Shape: \", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1BVvIvioyv2b"
   },
   "outputs": [],
   "source": [
    "x_dev = pd.get_dummies(x_dev, columns = one_hot_categorical, drop_first = True, dtype=int)\n",
    "x_test = pd.get_dummies(x_test, columns = one_hot_categorical, drop_first = True, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ff536pVeyv2b",
    "outputId": "7f95ec06-4378-4524-b76c-d0492a7d6a42"
   },
   "outputs": [],
   "source": [
    "print(\"x_dev Shape: \", x_dev.shape)\n",
    "print(\"x_test Shape: \", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols.append(\"Churn Score\")\n",
    "\n",
    "x_dev[numerical_cols] = x_dev[numerical_cols].apply(pd.to_numeric, errors='coerce')\n",
    "x_test[numerical_cols] = x_test[numerical_cols].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer \n",
    "\n",
    "# impute any missing value in our numerical column\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "x_dev[numerical_cols] = imputer.fit_transform(x_dev[numerical_cols])\n",
    "x_test[numerical_cols] = imputer.transform(x_test[numerical_cols]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# Make each numerical features normally distributed\n",
    "pt = PowerTransformer(method='yeo-johnson') \n",
    "x_dev[numerical_cols] = pt.fit_transform(x_dev[numerical_cols]) \n",
    "x_test[numerical_cols] = pt.transform(x_test[numerical_cols])   \n",
    "\n",
    "# x_dev[numerical_cols] = np.log1p(x_dev[numerical_cols]) \n",
    "# x_test[numerical_cols] = np.log1p(x_test[numerical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W91wejxZyv2b"
   },
   "outputs": [],
   "source": [
    "# Provided that we are using decision tree as our baseline and using Random Forest and XG Boost for our model, we will use Min Max Scaler\n",
    "# as that scale is best for these models\n",
    "minMax_Scaler = MinMaxScaler()\n",
    "x_dev[numerical_cols] = minMax_Scaler.fit_transform(x_dev[numerical_cols])\n",
    "x_test[numerical_cols] = minMax_Scaler.transform(x_test[numerical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "epwi_v6_yv2c",
    "outputId": "db9a8325-8947-44dc-d738-8fb2ba25f1dd"
   },
   "outputs": [],
   "source": [
    "# Check for any correlation\n",
    "correlation_matrix = x_dev.corr()\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sEQ2S0_Nyv2c",
    "outputId": "7b16e28c-d6e0-4c15-d522-681af46ea519"
   },
   "outputs": [],
   "source": [
    "# https://stackabuse.com/applying-filter-methods-in-python-for-feature-selection/\n",
    "# Find all features that return a threshold value greater than or equal to 0.8 to drop\n",
    "correlated_features = set()\n",
    "\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) >= 0.78:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            correlated_features.add(colname)\n",
    "\n",
    "print(correlated_features)\n",
    "print(len(correlated_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RDuM_OlTyv2c"
   },
   "outputs": [],
   "source": [
    "x_dev.drop(labels = correlated_features, axis = 1, inplace = True)\n",
    "x_test.drop(labels = correlated_features, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gDG-kshWyv2c",
    "outputId": "b4729264-b1b6-462a-e2c6-919459e76bc2"
   },
   "outputs": [],
   "source": [
    "correlation_matrix = x_dev.corr()\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 634
    },
    "id": "O_7jpIWKyv2d",
    "outputId": "954d9093-4626-43c2-b762-ab6f2eba8bfd"
   },
   "outputs": [],
   "source": [
    "# Generates heat map\n",
    "plt.imshow(correlation_matrix, cmap = 'coolwarm')\n",
    "\n",
    "# adding colorbar\n",
    "plt.colorbar()\n",
    "\n",
    "# gets the column name for tick labels\n",
    "tick_labels = [column for column in correlation_matrix.columns]\n",
    "\n",
    "# adds column label to the heat map \n",
    "plt.title(\"Correlation matrix for features\")\n",
    "plt.xticks(range(len(correlation_matrix)), tick_labels, rotation=50, ha='right', fontsize = 7)\n",
    "plt.yticks(range(len(correlation_matrix)), tick_labels, fontsize = 7)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IGLx8nYeyv2d",
    "outputId": "8080b57c-edbb-4628-f6c2-384a2477c769"
   },
   "outputs": [],
   "source": [
    "print(\"x_dev Shape: \", x_dev.shape)\n",
    "print(\"x_test Shape: \", x_test.shape, \"\\n\")\n",
    "\n",
    "print(\"y_dev Shape: \", y_dev.shape)\n",
    "print(\"y_test Shape: \", y_test.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adaYEJWpyv2e"
   },
   "source": [
    "# Model Building and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fuk0CEDTyv2e"
   },
   "source": [
    "## Decision Tree Baseline Model - Random Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 941
    },
    "id": "rxPPN6giz-C2",
    "outputId": "c4758a67-6b69-4a7e-8c8a-41d2a328765e"
   },
   "outputs": [],
   "source": [
    "ros = RandomOverSampler(random_state=42)\n",
    "x_resampled_ros, y_resampled_ros = ros.fit_resample(x_dev, y_dev)\n",
    "\n",
    "print(\"Class Distribution After Oversampling:\", Counter(y_resampled_ros))\n",
    "\n",
    "classifier = DecisionTreeClassifier(\n",
    "    max_depth=3,\n",
    "    min_samples_split=60,\n",
    "    min_samples_leaf=50,\n",
    "    max_features=\"sqrt\",\n",
    "    random_state=42,\n",
    "    ccp_alpha=0.1\n",
    ")\n",
    "\n",
    "scores = cross_validate(\n",
    "    classifier,\n",
    "    x_resampled_ros,\n",
    "    y_resampled_ros,\n",
    "    cv=5,\n",
    "    scoring=['roc_auc', 'average_precision']\n",
    ")\n",
    "\n",
    "classifier.fit(x_resampled_ros, y_resampled_ros)\n",
    "\n",
    "y_train_pred = classifier.predict(x_resampled_ros)\n",
    "y_train_prob = classifier.predict_proba(x_resampled_ros)[:, 1]\n",
    "\n",
    "roc_auc_train = roc_auc_score(y_resampled_ros, y_train_prob)\n",
    "print(f\"\\nTraining ROC AUC: {roc_auc_train:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report on Training Set:\")\n",
    "print(classification_report(y_resampled_ros, y_train_pred))\n",
    "\n",
    "y_test_pred = classifier.predict(x_test)\n",
    "y_test_prob = classifier.predict_proba(x_test)[:, 1]\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, y_test_prob)\n",
    "print(f\"Test ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report on Test Set:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=classifier.classes_)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Random Oversampling Model with Regularization\")\n",
    "plt.show()\n",
    "\n",
    "precision = precision_score(y_test, y_test_pred, pos_label=0)\n",
    "recall = recall_score(y_test, y_test_pred, pos_label=0)\n",
    "f1 = f1_score(y_test, y_test_pred, pos_label=0)\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OocIAlm52a5w"
   },
   "source": [
    "## Decision Tree Baseline Model - SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "id": "a2Ku6LZD2fvg",
    "outputId": "c094e844-4f21-401a-8a3e-9e8bc393466b"
   },
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "x_resampled_smote, y_resampled_smote = smote.fit_resample(x_dev, y_dev)\n",
    "print(\"SMOTE class distribution:\", Counter(y_resampled_smote))\n",
    "\n",
    "dt_smote = DecisionTreeClassifier(\n",
    "    max_depth=2,\n",
    "    min_samples_split=15,\n",
    "    min_samples_leaf=10,\n",
    "    max_features=\"sqrt\",\n",
    "    random_state=42,\n",
    "    ccp_alpha=0.05\n",
    ")\n",
    "\n",
    "scores = cross_validate(\n",
    "    dt_smote,\n",
    "    x_resampled_smote,\n",
    "    y_resampled_smote,\n",
    "    cv=5,\n",
    "    scoring=['roc_auc', 'average_precision']\n",
    ")\n",
    "\n",
    "dt_smote.fit(x_resampled_smote, y_resampled_smote)\n",
    "\n",
    "y_train_pred = dt_smote.predict(x_resampled_smote)\n",
    "y_train_prob = dt_smote.predict_proba(x_resampled_smote)[:, 1]\n",
    "roc_auc_train = roc_auc_score(y_resampled_smote, y_train_prob)\n",
    "\n",
    "print(f\"Training ROC AUC: {roc_auc_train:.4f}\")\n",
    "print(\"Classification Report on Training Set:\\n\", classification_report(y_resampled_smote, y_train_pred))\n",
    "\n",
    "y_pred_smote = dt_smote.predict(x_test)\n",
    "y_prob_smote = dt_smote.predict_proba(x_test)[:, 1]\n",
    "\n",
    "print(\"\\nDecision Tree with SMOTE Results on Test Set:\")\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_smote)\n",
    "print(\"Confusion Matrix on Test Set:\\n\", conf_matrix)\n",
    "print(\"Classification Report on Test Set:\\n\", classification_report(y_test, y_pred_smote))\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_smote)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=dt_smote.classes_)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix: Decision Tree with SMOTE\")\n",
    "plt.show()\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, y_prob_smote)\n",
    "precision = precision_score(y_test, y_pred_smote, pos_label=0)\n",
    "recall = recall_score(y_test, y_pred_smote, pos_label=0)\n",
    "f1 = f1_score(y_test, y_pred_smote, pos_label=0)\n",
    "\n",
    "print(f\"Test ROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model Baseline Model - RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "x_resampled_ros, y_resampled_ros = ros.fit_resample(x_dev, y_dev)\n",
    "\n",
    "# initial model\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100, \n",
    "    max_depth=10,      \n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,    \n",
    "    max_features=\"sqrt\",  \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# cross validation\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scores_rf = cross_validate(\n",
    "    rf_model,\n",
    "    x_resampled_ros,\n",
    "    y_resampled_ros,\n",
    "    cv=5,\n",
    "    scoring=['roc_auc', 'average_precision']\n",
    ")\n",
    "\n",
    "print(\"Random Forest CV Results:\")\n",
    "print(f\"Mean ROC AUC: {scores_rf['test_roc_auc'].mean():.4f}\")\n",
    "print(f\"Mean Average Precision: {scores_rf['test_average_precision'].mean():.4f}\")\n",
    "\n",
    "# model train\n",
    "rf_model.fit(x_resampled_ros, y_resampled_ros)\n",
    "\n",
    "# make prediction on development set\n",
    "y_dev_pred_rf = rf_model.predict(x_resampled_ros)\n",
    "y_dev_prob_rf = rf_model.predict_proba(x_resampled_ros)[:, 1]\n",
    "\n",
    "roc_auc_train_rf = roc_auc_score(y_resampled_ros, y_dev_prob_rf)\n",
    "print(f\"\\nTraining ROC AUC: {roc_auc_train_rf:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report on Training Set:\")\n",
    "print(classification_report(y_resampled_ros, y_dev_pred_rf))\n",
    "\n",
    "# make prediction on test set\n",
    "y_test_pred_rf = rf_model.predict(x_test)\n",
    "y_test_prob_rf = rf_model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "roc_auc_rf = roc_auc_score(y_test, y_test_prob_rf)\n",
    "print(f\"\\nTest ROC AUC: {roc_auc_rf:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report on Test Set:\")\n",
    "print(classification_report(y_test, y_test_pred_rf))\n",
    "\n",
    "# confusion matrix\n",
    "conf_matrix_rf = confusion_matrix(y_test, y_test_pred_rf)\n",
    "disp_rf = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_rf, display_labels=rf_model.classes_)\n",
    "disp_rf.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Random Forest Model\")\n",
    "plt.show()\n",
    "\n",
    "# performance\n",
    "precision_rf = precision_score(y_test, y_test_pred_rf, pos_label=0)\n",
    "recall_rf = recall_score(y_test, y_test_pred_rf, pos_label=0)\n",
    "f1_rf = f1_score(y_test, y_test_pred_rf, pos_label=0)\n",
    "print(f\"Precision: {precision_rf:.4f}, Recall: {recall_rf:.4f}, F1-Score: {f1_rf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature importance \n",
    "the result from initial model perform very well on development set and test set as the ROC AUC score and confusion matrix are equal to 1. So it might indicate that the model is overfitting. Therfore, we will do feature importance to remove or reevalute the importance of the feature in the model during the training.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': x_dev.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(feature_importances.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation between top three high importance score features and churn value\n",
    "correlation_with_target = x_dev[['Customer Status_ord', 'Satisfaction Score_ord', 'Churn Score']].join(y_dev).corr()\n",
    "print(correlation_with_target['Churn Value'])\n",
    "      \n",
    "print(df[['Churn Category', 'Churn Value']].groupby('Churn Category').mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove high importance feature based on correlation to reduce the risk of overfitting\n",
    "# remove churn category 1 since its not highly correlated with churn\n",
    "x_dev_dropped = x_dev.drop(columns=['Customer Status_ord', 'Churn Score', 'Churn Category_1'])\n",
    "x_test_dropped = x_test.drop(columns=['Customer Status_ord', 'Churn Score', 'Churn Category_1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-train model\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features=\"sqrt\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_model.fit(x_dev_dropped, y_dev)\n",
    "\n",
    "# predict test set\n",
    "y_test_pred_rf = rf_model.predict(x_test_dropped)\n",
    "y_test_prob_rf = rf_model.predict_proba(x_test_dropped)[:, 1]\n",
    "\n",
    "print(\"Test ROC AUC:\", roc_auc_score(y_test, y_test_prob_rf))\n",
    "print(\"\\nClassification Report on Test Set:\")\n",
    "print(classification_report(y_test, y_test_pred_rf))\n",
    "\n",
    "# confusion matrix\n",
    "conf_matrix_rf = confusion_matrix(y_test, y_test_pred_rf)\n",
    "disp_rf = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_rf, display_labels=rf_model.classes_)\n",
    "disp_rf.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Random Forest Model after Feature Adjustment\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model - SMOTE (comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with smote\n",
    "smote = SMOTE(random_state=42)\n",
    "x_resampled_smote, y_resampled_smote = smote.fit_resample(x_dev, y_dev)\n",
    "\n",
    "\n",
    "rf_smote_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features=\"sqrt\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_smote_model.fit(x_resampled_smote, y_resampled_smote)\n",
    "\n",
    "y_test_pred_rf_smote = rf_smote_model.predict(x_test)\n",
    "y_test_prob_rf_smote = rf_smote_model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "print(\"Random Forest with SMOTE Test ROC AUC:\", roc_auc_score(y_test, y_test_prob_rf_smote))\n",
    "print(\"\\nClassification Report on Test Set (Random Forest with SMOTE):\")\n",
    "print(classification_report(y_test, y_test_pred_rf_smote))\n",
    "\n",
    "conf_matrix_rf_smote = confusion_matrix(y_test, y_test_pred_rf_smote)\n",
    "disp_rf_smote = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_rf_smote, display_labels=rf_smote_model.classes_)\n",
    "disp_rf_smote.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Random Forest Model with SMOTE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': x_dev.columns,\n",
    "    'Importance': rf_smote_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(feature_importances.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " it performance same in random oversampling, so we will use x_dev_dropped and x_test_dropped in other later training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re- train model\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "x_resampled_smote_dropped, y_resampled_smote_dropped = smote.fit_resample(x_dev_dropped, y_dev)\n",
    "\n",
    "rf_smote_drop_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features=\"sqrt\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_smote_drop_model.fit(x_resampled_smote_dropped, y_resampled_smote_dropped)\n",
    "\n",
    "y_test_pred_drop = rf_smote_drop_model.predict(x_test_dropped)\n",
    "y_test_prob_drop = rf_smote_drop_model.predict_proba(x_test_dropped)[:, 1]\n",
    "\n",
    "print(\"Test ROC AUC after Removing Leak Features:\", roc_auc_score(y_test, y_test_prob_drop))\n",
    "print(\"\\nClassification Report after Removing Leak Features:\")\n",
    "print(classification_report(y_test, y_test_pred_drop))\n",
    "\n",
    "conf_matrix_cleaned = confusion_matrix(y_test, y_test_pred_drop)\n",
    "disp_cleaned = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_cleaned, display_labels=rf_smote_drop_model.classes_)\n",
    "disp_cleaned.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Random Forest Model after Removing Leak Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check if the model wont highly dependent on one feature\n",
    "# recalculate the feature importance\n",
    "feature_importances_smote_drop = pd.DataFrame({\n",
    "    'Feature': x_dev_dropped.columns,\n",
    "    'Importance': rf_smote_drop_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importances after Removing Leak Features:\")\n",
    "print(feature_importances_smote_drop.head(10))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve - compared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fpr_rf_smote, tpr_rf_smote, _ = roc_curve(y_test, y_test_prob_drop)\n",
    "roc_auc_rf_smote = auc(fpr_rf_smote, tpr_rf_smote)\n",
    "fpr_rf_ros, tpr_rf_ros, _ = roc_curve(y_test, y_test_prob_rf)\n",
    "roc_auc_rf_ros = auc(fpr_rf_ros, tpr_rf_ros)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr_rf_smote, tpr_rf_smote, label=f\"Random Forest- SMOTE ROC AUC = {roc_auc_rf_smote:.4f}\", linewidth=2)\n",
    "plt.plot(fpr_rf_ros, tpr_rf_ros, label=f\"Random Forest- ROS ROC AUC = {roc_auc_rf_ros:.4f}\", linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label=\"Random Guess\")\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=12)\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=12)\n",
    "plt.title(\"Random Forest ROC Curve Comparison: SMOTE vs ROS\", fontsize=14)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5Dxc2Gwyv2e"
   },
   "source": [
    "## XG Boost Model"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
